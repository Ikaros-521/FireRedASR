#!/usr/bin/env python3

import os
import sys
import time
import traceback
import gradio as gr
import argparse
import soundfile as sf
import gc
import torch
import psutil
from fireredasr.models.fireredasr import FireRedAsr
from fireredasr.data.asr_feat import ASRFeatExtractor
from fireredasr.tokenizer.llm_tokenizer import LlmTokenizerWrapper

# å…¨å±€å˜é‡ç”¨äºç¼“å­˜æ¨¡å‹
cached_model = None
cached_model_type = None
cached_model_dir = None

def get_memory_usage():
    """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    mem_usage_mb = mem_info.rss / 1024 / 1024  # è½¬æ¢ä¸ºMB
    
    gpu_mem_used = 0
    if torch.cuda.is_available():
        gpu_mem_used = torch.cuda.memory_allocated() / 1024 / 1024  # è½¬æ¢ä¸ºMB
    
    return f"å†…å­˜: {mem_usage_mb:.2f}MB, GPUæ˜¾å­˜: {gpu_mem_used:.2f}MB"

def clear_model_cache():
    """æ¸…ç†æ¨¡å‹ç¼“å­˜"""
    global cached_model, cached_model_type, cached_model_dir
    
    if cached_model is not None:
        print("æ­£åœ¨æ¸…ç†æ¨¡å‹ç¼“å­˜...")
        before_mem = get_memory_usage()
        
        del cached_model
        cached_model = None
        cached_model_type = None
        cached_model_dir = None
        
        # å¼ºåˆ¶æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        after_mem = get_memory_usage()
        print(f"æ¨¡å‹ç¼“å­˜å·²æ¸…ç†ï¼Œå†…å­˜ä½¿ç”¨å‰: {before_mem}, å†…å­˜ä½¿ç”¨å: {after_mem}")
        return f"æ¨¡å‹ç¼“å­˜å·²æ¸…ç†\n{after_mem}"
    else:
        mem_info = get_memory_usage()
        print(f"æ²¡æœ‰ç¼“å­˜çš„æ¨¡å‹éœ€è¦æ¸…ç†ï¼Œå½“å‰å†…å­˜ä½¿ç”¨: {mem_info}")
        return f"æ²¡æœ‰ç¼“å­˜çš„æ¨¡å‹éœ€è¦æ¸…ç†\n{mem_info}"

def get_model(asr_type, model_dir):
    """è·å–æ¨¡å‹ï¼Œå¦‚æœç±»å‹å’Œç›®å½•ç›¸åŒåˆ™ä½¿ç”¨ç¼“å­˜"""
    global cached_model, cached_model_type, cached_model_dir
    
    # å¦‚æœæ¨¡å‹ç±»å‹æˆ–ç›®å½•å‘ç”Ÿå˜åŒ–ï¼Œæ¸…ç†ç¼“å­˜
    if cached_model is not None and (cached_model_type != asr_type or cached_model_dir != model_dir):
        print(f"æ¨¡å‹ç±»å‹æˆ–ç›®å½•å·²å˜æ›´ï¼Œæ¸…ç†ç¼“å­˜...")
        print(f"ä¹‹å‰: {cached_model_type} - {cached_model_dir}")
        print(f"å½“å‰: {asr_type} - {model_dir}")
        clear_model_cache()
    
    # å¦‚æœæ²¡æœ‰ç¼“å­˜æ¨¡å‹ï¼ŒåŠ è½½æ–°æ¨¡å‹
    if cached_model is None:
        print(f"åŠ è½½æ–°æ¨¡å‹: {asr_type} - {model_dir}")
        cached_model = FireRedAsr.from_pretrained(asr_type, model_dir)
        cached_model_type = asr_type
        cached_model_dir = model_dir
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œå½“å‰å†…å­˜ä½¿ç”¨: {get_memory_usage()}")
    else:
        print(f"ä½¿ç”¨ç¼“å­˜æ¨¡å‹: {cached_model_type} - {cached_model_dir}")
    
    return cached_model

def test_audio_processing(audio_path):
    """æµ‹è¯•éŸ³é¢‘å¤„ç†æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    try:
        if not audio_path:
            return False, "è¯·å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"
            
        print(f"æµ‹è¯•éŸ³é¢‘å¤„ç†: {audio_path}")
        
        # ä½¿ç”¨soundfileæ£€æŸ¥éŸ³é¢‘
        print("ä½¿ç”¨soundfileè¯»å–éŸ³é¢‘...")
        data, samplerate = sf.read(audio_path)
        print(f"éŸ³é¢‘ä¿¡æ¯: é‡‡æ ·ç‡={samplerate}Hz, å½¢çŠ¶={data.shape}, æ—¶é•¿={len(data)/samplerate:.2f}ç§’")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å•å£°é“ï¼Œå¦‚æœä¸æ˜¯åˆ™è½¬æ¢
        if len(data.shape) > 1 and data.shape[1] > 1:
            print(f"æ£€æµ‹åˆ°å¤šå£°é“éŸ³é¢‘({data.shape[1]}å£°é“)ï¼Œè½¬æ¢ä¸ºå•å£°é“")
            mono_data = data.mean(axis=1)
            temp_path = audio_path + ".mono.wav"
            sf.write(temp_path, mono_data, samplerate)
            print(f"å·²ä¿å­˜å•å£°é“éŸ³é¢‘åˆ°: {temp_path}")
            audio_path = temp_path
        
        # ä½¿ç”¨ASRFeatExtractoræµ‹è¯•ç‰¹å¾æå–
        print("æµ‹è¯•ç‰¹å¾æå–...")
        cmvn_path = os.path.join("pretrained_models", os.listdir("pretrained_models")[0], "cmvn.ark")
        if os.path.exists(cmvn_path):
            feat_extractor = ASRFeatExtractor(cmvn_path)
            feats, lengths, durs = feat_extractor([audio_path])
            print(f"ç‰¹å¾æå–æˆåŠŸ: ç‰¹å¾å½¢çŠ¶={feats.shape}, é•¿åº¦={lengths}, æ—¶é•¿={durs}ç§’")
            return True, f"éŸ³é¢‘æµ‹è¯•æˆåŠŸ: é‡‡æ ·ç‡={samplerate}Hz, æ—¶é•¿={durs[0]:.2f}ç§’, ç‰¹å¾å½¢çŠ¶={feats.shape}"
        else:
            return False, f"æ‰¾ä¸åˆ°CMVNæ–‡ä»¶: {cmvn_path}"
    
    except Exception as e:
        error_msg = f"éŸ³é¢‘æµ‹è¯•å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        print(error_msg)
        return False, error_msg

def safe_model_to_device(model, use_gpu):
    """å®‰å…¨åœ°å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼Œå¤„ç†meta tensoré—®é¢˜"""
    if not use_gpu:
        try:
            model.cpu()
            return model
        except Exception as e:
            print(f"å°†æ¨¡å‹ç§»åŠ¨åˆ°CPUæ—¶å‡ºé”™: {str(e)}")
            print("å°è¯•ä½¿ç”¨æ›¿ä»£æ–¹æ³•...")
    
    if use_gpu and torch.cuda.is_available():
        try:
            # å°è¯•ç›´æ¥ç§»åŠ¨åˆ°CUDA
            model.cuda()
        except NotImplementedError as e:
            if "Cannot copy out of meta tensor" in str(e):
                print("æ£€æµ‹åˆ°meta tensoré—®é¢˜ï¼Œä½¿ç”¨ç‰¹æ®Šå¤„ç†...")
                # å¯¹äºLLMæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ç‰¹æ®Šå¤„ç†
                if hasattr(model, 'model') and hasattr(model.model, 'to'):
                    try:
                        # å°è¯•ä½¿ç”¨assign=Trueå‚æ•°
                        print("å°è¯•ä½¿ç”¨assign=Trueå‚æ•°...")
                        for param in model.parameters():
                            if hasattr(param, 'is_meta') and param.is_meta:
                                print(f"è·³è¿‡metaå‚æ•°: {param.shape}")
                        
                        # å¯¹äºLLMæ¨¡å‹ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦è·³è¿‡æŸäº›å‚æ•°çš„ç§»åŠ¨
                        if hasattr(model, 'transcribe'):
                            # ä½¿ç”¨åŸå§‹æ–¹æ³•ï¼Œä½†è·³è¿‡meta tensor
                            original_transcribe = model.transcribe
                            def safe_transcribe(*args, **kwargs):
                                # ç¡®ä¿è¾“å…¥æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                                if len(args) >= 3 and torch.is_tensor(args[0]):
                                    args = list(args)
                                    args[0] = args[0].cuda()
                                    args[1] = args[1].cuda()
                                    args = tuple(args)
                                return original_transcribe(*args, **kwargs)
                            model.transcribe = safe_transcribe
                    except Exception as inner_e:
                        print(f"ç‰¹æ®Šå¤„ç†å¤±è´¥: {str(inner_e)}")
            else:
                print(f"å°†æ¨¡å‹ç§»åŠ¨åˆ°CUDAæ—¶å‡ºé”™: {str(e)}")
    
    return model

def transcribe_audio(audio_path, asr_type, model_dir, use_gpu, beam_size, 
                     batch_size=1, nbest=1, decode_max_len=0, 
                     softmax_smoothing=1.0, aed_length_penalty=0.0, eos_penalty=1.0,
                     decode_min_len=0, repetition_penalty=1.0, llm_length_penalty=0.0, temperature=1.0):
    """è½¬å†™éŸ³é¢‘æ–‡ä»¶"""
    try:
        if not audio_path:
            return "è¯·å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"
            
        print(f"å¼€å§‹å¤„ç†éŸ³é¢‘: {audio_path}")
        print(f"å‚æ•°: asr_type={asr_type}, model_dir={model_dir}, use_gpu={use_gpu}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(audio_path):
            return f"é”™è¯¯: éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ - {audio_path}"
            
        # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_dir):
            return f"é”™è¯¯: æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ - {model_dir}"
            
        # æ£€æŸ¥æ¨¡å‹ç›®å½•ä¸­çš„å¿…è¦æ–‡ä»¶
        required_files = ["cmvn.ark"]
        if asr_type == "aed":
            required_files.extend(["model.pth.tar", "dict.txt", "train_bpe1000.model"])
        elif asr_type == "llm":
            required_files.extend(["model.pth.tar", "asr_encoder.pth.tar"])
            if not os.path.exists(os.path.join(model_dir, "Qwen2-7B-Instruct")):
                return f"é”™è¯¯: LLMæ¨¡å‹ç›®å½•ä¸å­˜åœ¨ - {os.path.join(model_dir, 'Qwen2-7B-Instruct')}"
                
        for file in required_files:
            if not os.path.exists(os.path.join(model_dir, file)):
                return f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ - {os.path.join(model_dir, file)}"
        
        # æ£€æŸ¥éŸ³é¢‘æ ¼å¼å¹¶å¤„ç†
        try:
            data, samplerate = sf.read(audio_path)
            print(f"éŸ³é¢‘ä¿¡æ¯: é‡‡æ ·ç‡={samplerate}Hz, å½¢çŠ¶={data.shape}, æ—¶é•¿={len(data)/samplerate:.2f}ç§’")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å•å£°é“ï¼Œå¦‚æœä¸æ˜¯åˆ™è½¬æ¢
            if len(data.shape) > 1 and data.shape[1] > 1:
                print(f"æ£€æµ‹åˆ°å¤šå£°é“éŸ³é¢‘({data.shape[1]}å£°é“)ï¼Œè½¬æ¢ä¸ºå•å£°é“")
                mono_data = data.mean(axis=1)
                temp_path = audio_path + ".mono.wav"
                sf.write(temp_path, mono_data, samplerate)
                print(f"å·²ä¿å­˜å•å£°é“éŸ³é¢‘åˆ°: {temp_path}")
                audio_path = temp_path
        except Exception as e:
            print(f"éŸ³é¢‘å¤„ç†è­¦å‘Š: {str(e)}")
            print("ç»§ç»­ä½¿ç”¨åŸå§‹éŸ³é¢‘...")
        
        print("è·å–æ¨¡å‹ä¸­...")
        # è·å–æ¨¡å‹ï¼ˆä½¿ç”¨ç¼“å­˜æˆ–åŠ è½½æ–°æ¨¡å‹ï¼‰
        model = get_model(asr_type, model_dir)
        
        # å‡†å¤‡å‚æ•°
        uttid = os.path.basename(audio_path).replace(".wav", "")
        
        # è®¾ç½®å‚æ•°
        params = {
            "use_gpu": use_gpu,
            "beam_size": beam_size,
            "nbest": nbest,
            "decode_max_len": decode_max_len,
            "softmax_smoothing": softmax_smoothing,
            "aed_length_penalty": aed_length_penalty,
            "eos_penalty": eos_penalty,
            "decode_min_len": decode_min_len,
            "repetition_penalty": repetition_penalty,
            "llm_length_penalty": llm_length_penalty,
            "temperature": temperature
        }
        
        # å¤„ç†æ¨¡å‹åˆ°è®¾å¤‡çš„ç§»åŠ¨
        if asr_type == "llm" and use_gpu:
            print("æ£€æµ‹åˆ°LLMæ¨¡å‹ï¼Œä½¿ç”¨ç‰¹æ®Šå¤„ç†...")
            # å¯¹äºLLMæ¨¡å‹ï¼Œæˆ‘ä»¬ç›´æ¥åœ¨åŸå§‹çš„transcribeæ–¹æ³•ä¸­å¤„ç†è®¾å¤‡ç§»åŠ¨
            # ä¿®æ”¹FireRedAsr.transcribeæ–¹æ³•ä¸­çš„è®¾å¤‡å¤„ç†é€»è¾‘
            feats, lengths, durs = model.feat_extractor([audio_path])
            total_dur = sum(durs)
            
            if use_gpu:
                feats, lengths = feats.cuda(), lengths.cuda()
                # ä¸è°ƒç”¨model.cuda()ï¼Œé¿å…meta tensoré”™è¯¯
            else:
                feats, lengths = feats.cpu(), lengths.cpu()
                model.model.cpu()
            
            if asr_type == "llm":
                # ä½¿ç”¨LlmTokenizerWrapperç±»çš„é™æ€æ–¹æ³•ï¼Œè€Œä¸æ˜¯ç›´æ¥è°ƒç”¨tokenizerçš„æ–¹æ³•
                input_ids, attention_mask, _, _ = LlmTokenizerWrapper.preprocess_texts(
                    origin_texts=[""] * feats.size(0), 
                    tokenizer=model.tokenizer, 
                    max_len=128, 
                    decode=True
                )
                if use_gpu:
                    input_ids = input_ids.cuda()
                    attention_mask = attention_mask.cuda()
                
                print("å¼€å§‹LLMè½¬å†™...")
                start_time = time.time() if not torch.cuda.is_available() else None
                if torch.cuda.is_available():
                    start_event = torch.cuda.Event(enable_timing=True)
                    end_event = torch.cuda.Event(enable_timing=True)
                    start_event.record()
                
                try:
                    generated_ids = model.model.transcribe(
                        feats, lengths, input_ids, attention_mask,
                        params.get("beam_size", 1),
                        params.get("decode_max_len", 0),
                        params.get("decode_min_len", 0),
                        params.get("repetition_penalty", 1.0),
                        params.get("llm_length_penalty", 0.0),
                        params.get("temperature", 1.0)
                    )
                except Exception as e:
                    print(f"LLMè½¬å†™å‡ºé”™: {str(e)}")
                    print("å°è¯•ä½¿ç”¨åŸå§‹transcribeæ–¹æ³•...")
                    # å¦‚æœç›´æ¥è°ƒç”¨model.model.transcribeå¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹çš„transcribeæ–¹æ³•
                    return "LLMæ¨¡å‹è½¬å†™å¤±è´¥ï¼Œè¯·å°è¯•ä½¿ç”¨AEDæ¨¡å¼æˆ–æ£€æŸ¥æ¨¡å‹é…ç½®ã€‚\nè¯¦ç»†é”™è¯¯: " + str(e)
                
                if torch.cuda.is_available():
                    end_event.record()
                    torch.cuda.synchronize()
                    elapsed = start_event.elapsed_time(end_event) / 1000.0
                else:
                    elapsed = time.time() - start_time
                
                rtf = elapsed / total_dur if total_dur > 0 else 0
                texts = model.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
                results = []
                for uid, wav, text in zip([uttid], [audio_path], texts):
                    results.append({"uttid": uid, "text": text, "wav": wav, "rtf": f"{rtf:.4f}"})
                
                # ç”Ÿæˆè¾“å‡º
                output = ""
                for result in results:
                    output += f"éŸ³é¢‘ID: {result['uttid']}\n"
                    output += f"è¯†åˆ«ç»“æœ: {result['text']}\n"
                    output += f"å®æ—¶ç‡(RTF): {result['rtf']}\n"
                    
                print("è½¬å†™å®Œæˆ")
                return output
        
        print(f"å¼€å§‹è½¬å†™: uttid={uttid}, å‚æ•°={params}")
        # æ‰§è¡Œè½¬å†™
        results = model.transcribe([uttid], [audio_path], params)
        
        # ç”Ÿæˆè¾“å‡º
        output = ""
        for result in results:
            output += f"éŸ³é¢‘ID: {result['uttid']}\n"
            output += f"è¯†åˆ«ç»“æœ: {result['text']}\n"
            output += f"å®æ—¶ç‡(RTF): {result['rtf']}\n"
            
        print("è½¬å†™å®Œæˆ")
        return output
    
    except Exception as e:
        error_msg = f"å‘ç”Ÿé”™è¯¯: {str(e)}\n\nè¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg

def load_audio(audio_path):
    """åŠ è½½éŸ³é¢‘æ–‡ä»¶åˆ°ç•Œé¢é¢„è§ˆ"""
    if audio_path and os.path.exists(audio_path):
        return audio_path
    return None

def on_asr_type_change(asr_type):
    """å½“ASRç±»å‹æ”¹å˜æ—¶æ‰§è¡Œçš„æ“ä½œ"""
    clear_model_cache()
    return f"å·²åˆ‡æ¢åˆ° {asr_type} æ¨¡å¼ï¼Œæ¨¡å‹ç¼“å­˜å·²æ¸…ç†\nå½“å‰å†…å­˜ä½¿ç”¨: {get_memory_usage()}"

def update_cache_status():
    """æ›´æ–°ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
    global cached_model, cached_model_type, cached_model_dir
    
    if cached_model is None:
        return f"æ¨¡å‹æœªåŠ è½½\n{get_memory_usage()}"
    else:
        return f"å·²åŠ è½½æ¨¡å‹: {cached_model_type} - {os.path.basename(cached_model_dir)}\n{get_memory_usage()}"

def create_interface():
    # è·å–é¢„è®­ç»ƒæ¨¡å‹ç›®å½•
    pretrained_dirs = []
    if os.path.exists("pretrained_models"):
        pretrained_dirs = [d for d in os.listdir("pretrained_models") if os.path.isdir(os.path.join("pretrained_models", d))]
        print(f"æ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ç›®å½•: {pretrained_dirs}")
    else:
        print("é¢„è®­ç»ƒæ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
    
    with gr.Blocks(title="FireRedASR è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ") as demo:
        gr.Markdown("# ğŸ”¥ FireRedASR è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ")
        gr.Markdown("ä¸Šä¼ WAVéŸ³é¢‘æ–‡ä»¶å¹¶é…ç½®å‚æ•°è¿›è¡Œè¯­éŸ³è¯†åˆ«")
        
        cache_status = gr.Textbox(label="ç¼“å­˜çŠ¶æ€", value="æ¨¡å‹æœªåŠ è½½\n" + get_memory_usage(), lines=2)
        
        with gr.Row():
            with gr.Column(scale=1):
                # æ ¸å¿ƒå‚æ•° - æ˜¾çœ¼æ˜¾ç¤º
                gr.Markdown("## æ ¸å¿ƒå‚æ•°")
                audio_file = gr.Audio(
                    label="ä¸Šä¼ æˆ–å½•åˆ¶éŸ³é¢‘ï¼ˆæ”¯æŒWAVä¸Šä¼ æˆ–ç›´æ¥å½•éŸ³ï¼‰",
                    sources=["upload", "microphone"],
                    type="filepath"
                )
                # audio_player = gr.Audio(label="éŸ³é¢‘é¢„è§ˆ", type="filepath", visible=True)
                asr_type = gr.Radio(choices=["aed", "llm"], label="ASRç±»å‹ï¼ˆè¯·æ ¹æ®å®é™…ä½¿ç”¨æ¨¡å‹åˆ‡æ¢ï¼‰", value="aed")
                
                gr.Markdown("""
                > **æç¤º**: 
                > - AEDæ¨¡å¼é€‚åˆä¸€èˆ¬è¯­éŸ³è¯†åˆ«ä»»åŠ¡
                > - LLMæ¨¡å¼é€‚åˆé•¿æ–‡æœ¬å’Œå¤æ‚è¯­å¢ƒ
                > - åˆ‡æ¢æ¨¡å‹ç±»å‹ä¼šè‡ªåŠ¨æ¸…ç†ç¼“å­˜
                """)
                
                if pretrained_dirs:
                    model_dir = gr.Dropdown(choices=[os.path.join("pretrained_models", d) for d in pretrained_dirs], 
                                           label="é¢„è®­ç»ƒæ¨¡å‹ç›®å½•", 
                                           value=os.path.join("pretrained_models", pretrained_dirs[0]) if pretrained_dirs else None)
                else:
                    model_dir = gr.Textbox(label="é¢„è®­ç»ƒæ¨¡å‹ç›®å½•", placeholder="è¾“å…¥æ¨¡å‹ç›®å½•è·¯å¾„")
                
                use_gpu = gr.Checkbox(label="ä½¿ç”¨GPU", value=True)
                beam_size = gr.Slider(minimum=1, maximum=10, value=1, step=1, label="Beam Size")
                
                # æŠ˜å çš„å…¶ä»–å‚æ•°
                with gr.Accordion("é«˜çº§å‚æ•°", open=False):
                    batch_size = gr.Slider(minimum=1, maximum=8, value=1, step=1, label="Batch Size")
                    
                    gr.Markdown("### FireRedASR-AED å‚æ•°")
                    nbest = gr.Slider(minimum=1, maximum=10, value=1, step=1, label="N-best")
                    softmax_smoothing = gr.Slider(minimum=0.1, maximum=2.0, value=1.0, step=0.1, label="Softmax Smoothing")
                    aed_length_penalty = gr.Slider(minimum=0.0, maximum=2.0, value=0.0, step=0.1, label="AED Length Penalty")
                    eos_penalty = gr.Slider(minimum=0.0, maximum=2.0, value=1.0, step=0.1, label="EOS Penalty")
                    
                    gr.Markdown("### FireRedASR-LLM å‚æ•°")
                    decode_max_len = gr.Slider(minimum=0, maximum=200, value=0, step=10, label="Decode Max Length")
                    decode_min_len = gr.Slider(minimum=0, maximum=50, value=0, step=5, label="Decode Min Length")
                    repetition_penalty = gr.Slider(minimum=1.0, maximum=2.0, value=1.0, step=0.1, label="Repetition Penalty")
                    llm_length_penalty = gr.Slider(minimum=0.0, maximum=2.0, value=0.0, step=0.1, label="LLM Length Penalty")
                    temperature = gr.Slider(minimum=0.1, maximum=2.0, value=1.0, step=0.1, label="Temperature")
                
                with gr.Row():
                    submit_btn = gr.Button("å¼€å§‹è¯†åˆ«", variant="primary")
                    test_btn = gr.Button("æµ‹è¯•éŸ³é¢‘", variant="secondary")
                    clear_cache_btn = gr.Button("æ¸…ç†æ¨¡å‹ç¼“å­˜", variant="secondary")
                    refresh_status_btn = gr.Button("åˆ·æ–°çŠ¶æ€", variant="secondary")
            
            with gr.Column(scale=1):
                output_text = gr.Textbox(label="è¯†åˆ«ç»“æœ", lines=10)
        
        # è®¾ç½®éŸ³é¢‘åŠ è½½äº‹ä»¶
        # audio_file.change(
        #     fn=load_audio,
        #     inputs=[audio_file],
        #     outputs=[audio_player]
        # )
        
        # è®¾ç½®ASRç±»å‹å˜åŒ–äº‹ä»¶
        asr_type.change(
            fn=on_asr_type_change,
            inputs=[asr_type],
            outputs=[cache_status]
        )
        
        # è®¾ç½®æ¸…ç†ç¼“å­˜äº‹ä»¶
        clear_cache_btn.click(
            fn=clear_model_cache,
            inputs=[],
            outputs=[cache_status]
        )
        
        # è®¾ç½®åˆ·æ–°çŠ¶æ€äº‹ä»¶
        refresh_status_btn.click(
            fn=update_cache_status,
            inputs=[],
            outputs=[cache_status]
        )
        
        # è®¾ç½®æäº¤äº‹ä»¶
        submit_btn.click(
            fn=transcribe_audio,
            inputs=[
                audio_file, asr_type, model_dir, use_gpu, beam_size,
                batch_size, nbest, decode_max_len, 
                softmax_smoothing, aed_length_penalty, eos_penalty,
                decode_min_len, repetition_penalty, llm_length_penalty, temperature
            ],
            outputs=output_text
        )
        
        # è®¾ç½®æµ‹è¯•äº‹ä»¶
        test_btn.click(
            fn=test_audio_processing,
            inputs=[audio_file],
            outputs=output_text
        )
        
        # ç¤ºä¾‹
        example_dir = "examples/wav" if os.path.exists("examples/wav") else None
        if example_dir and os.path.exists(example_dir):
            example_files = [os.path.join(example_dir, f) for f in os.listdir(example_dir) if f.endswith('.wav')]
            if example_files:
                gr.Examples(
                    examples=example_files,
                    inputs=audio_file,
                    label="ç¤ºä¾‹éŸ³é¢‘"
                )
    
    return demo

if __name__ == "__main__":
    print("å¯åŠ¨FireRedASR Gradioç•Œé¢...")
    demo = create_interface()
    demo.launch(share=False) 